{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":9215002,"sourceType":"datasetVersion","datasetId":5572157}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchaudio\nfrom transformers import AutoConfig, Wav2Vec2Processor\n\nimport librosa\nimport IPython.display as ipd\nimport numpy as np\nimport pandas as pd\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T04:03:51.490354Z","iopub.execute_input":"2024-08-21T04:03:51.491527Z","iopub.status.idle":"2024-08-21T04:03:51.497116Z","shell.execute_reply.started":"2024-08-21T04:03:51.491485Z","shell.execute_reply":"2024-08-21T04:03:51.495961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def speech_file_to_array_fn(path, sampling_rate):\n    speech_array, _sampling_rate = torchaudio.load(path)\n    resampler = torchaudio.transforms.Resample(_sampling_rate)\n    speech = resampler(speech_array).squeeze().numpy()\n    return speech\n\n\ndef predict(path, sampling_rate):\n    speech = speech_file_to_array_fn(path, sampling_rate)\n    features = processor_new(speech, sampling_rate=sampling_rate, return_tensors=\"pt\", padding=True)\n\n    input_values = features.input_values.to(device)\n    attention_mask = features.attention_mask.to(device)\n\n    with torch.no_grad():\n        logits = model_new(input_values, attention_mask=attention_mask).logits\n\n    scores = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n    outputs = [{\"Emotion\": config_new.id2label[i], \"Score\": f\"{round(score * 100, 3):.1f}%\"} for i, score in enumerate(scores)]\n    return outputs\n\n\nSTYLES = \"\"\"\n<style>\ndiv.display_data {\n    margin: 0 auto;\n    max-width: 500px;\n}\ntable.xxx {\n    margin: 50px !important;\n    float: right !important;\n    clear: both !important;\n}\ntable.xxx td {\n    min-width: 300px !important;\n    text-align: center !important;\n}\n</style>\n\"\"\".strip()\n\ndef prediction(df_row):\n    path, emotion = df_row[\"path\"], df_row[\"emotion\"]\n    df = pd.DataFrame([{\"Emotion\": emotion, \"Sentence\": \"    \"}])\n    setup = {\n        'border': 2,\n        'show_dimensions': True,\n        'justify': 'center',\n        'classes': 'xxx',\n        'escape': False,\n    }\n    ipd.display(ipd.HTML(STYLES + df.to_html(**setup) + \"<br />\"))\n    speech, sr = torchaudio.load(path)\n    speech = speech[0].numpy().squeeze()\n    speech = librosa.resample(np.asarray(speech), orig_sr=sr, target_sr=sampling_rate)\n    ipd.display(ipd.Audio(data=np.asarray(speech), autoplay=True, rate=sampling_rate))\n\n    outputs = predict(path, sampling_rate)\n    r = pd.DataFrame(outputs)\n    ipd.display(ipd.HTML(STYLES + r.to_html(**setup) + \"<br />\"))","metadata":{"execution":{"iopub.status.busy":"2024-08-21T04:16:53.098866Z","iopub.execute_input":"2024-08-21T04:16:53.099348Z","iopub.status.idle":"2024-08-21T04:16:53.112753Z","shell.execute_reply.started":"2024-08-21T04:16:53.099294Z","shell.execute_reply":"2024-08-21T04:16:53.111289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_path = '/kaggle/input/test-speech/test.csv'\ntest = pd.read_csv(test_path, sep=\"\\t\")\ntest.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-21T04:16:53.274880Z","iopub.execute_input":"2024-08-21T04:16:53.275272Z","iopub.status.idle":"2024-08-21T04:16:53.294523Z","shell.execute_reply.started":"2024-08-21T04:16:53.275244Z","shell.execute_reply":"2024-08-21T04:16:53.293386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from dataclasses import dataclass\nfrom typing import Optional, Tuple\nimport torch\nfrom transformers.file_utils import ModelOutput\n\n\n@dataclass\nclass SpeechClassifierOutput(ModelOutput):\n    loss: Optional[torch.FloatTensor] = None\n    logits: torch.FloatTensor = None\n    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n    attentions: Optional[Tuple[torch.FloatTensor]] = None","metadata":{"execution":{"iopub.status.busy":"2024-08-21T04:16:53.463045Z","iopub.execute_input":"2024-08-21T04:16:53.463464Z","iopub.status.idle":"2024-08-21T04:16:53.471489Z","shell.execute_reply.started":"2024-08-21T04:16:53.463434Z","shell.execute_reply":"2024-08-21T04:16:53.469972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n\nfrom transformers.models.wav2vec2.modeling_wav2vec2 import (\n    Wav2Vec2PreTrainedModel,\n    Wav2Vec2Model\n)\n\n\nclass Wav2Vec2ClassificationHead(nn.Module):\n    \"\"\"Head for wav2vec classification task.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n        self.dropout = nn.Dropout(config.final_dropout)\n        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n\n    def forward(self, features, **kwargs):\n        x = features\n        x = self.dropout(x)\n        x = self.dense(x)\n        x = torch.tanh(x)\n        x = self.dropout(x)\n        x = self.out_proj(x)\n        return x\n\n\nclass Wav2Vec2ForSpeechClassification(Wav2Vec2PreTrainedModel):\n    def __init__(self, config):\n        super().__init__(config)\n        self.num_labels = config.num_labels\n        self.pooling_mode = config.pooling_mode\n        self.config = config\n\n        self.wav2vec2 = Wav2Vec2Model(config)\n        self.classifier = Wav2Vec2ClassificationHead(config)\n\n        self.init_weights()\n\n    def freeze_feature_extractor(self):\n        self.wav2vec2.feature_extractor._freeze_parameters()\n\n    def merged_strategy(\n            self,\n            hidden_states,\n            mode=\"mean\"\n    ):\n        if mode == \"mean\":\n            outputs = torch.mean(hidden_states, dim=1)\n        elif mode == \"sum\":\n            outputs = torch.sum(hidden_states, dim=1)\n        elif mode == \"max\":\n            outputs = torch.max(hidden_states, dim=1)[0]\n        else:\n            raise Exception(\n                \"The pooling method hasn't been defined! Your pooling mode must be one of these ['mean', 'sum', 'max']\")\n\n        return outputs\n\n    def forward(\n            self,\n            input_values,\n            attention_mask=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n            labels=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.wav2vec2(\n            input_values,\n            attention_mask=attention_mask,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        hidden_states = outputs[0]\n        hidden_states = self.merged_strategy(hidden_states, mode=self.pooling_mode)\n        logits = self.classifier(hidden_states)\n\n        loss = None\n        if labels is not None:\n            if self.config.problem_type is None:\n                if self.num_labels == 1:\n                    self.config.problem_type = \"regression\"\n                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n                    self.config.problem_type = \"single_label_classification\"\n                else:\n                    self.config.problem_type = \"multi_label_classification\"\n\n            if self.config.problem_type == \"regression\":\n                loss_fct = MSELoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels)\n            elif self.config.problem_type == \"single_label_classification\":\n                loss_fct = CrossEntropyLoss()\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n            elif self.config.problem_type == \"multi_label_classification\":\n                loss_fct = BCEWithLogitsLoss()\n                loss = loss_fct(logits, labels)\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return SpeechClassifierOutput(\n            loss=loss,\n            logits=logits,\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )\n","metadata":{"execution":{"iopub.status.busy":"2024-08-21T04:16:53.675718Z","iopub.execute_input":"2024-08-21T04:16:53.676111Z","iopub.status.idle":"2024-08-21T04:16:53.695459Z","shell.execute_reply.started":"2024-08-21T04:16:53.676084Z","shell.execute_reply":"2024-08-21T04:16:53.694087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Device: {device}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-21T04:16:54.229352Z","iopub.execute_input":"2024-08-21T04:16:54.230520Z","iopub.status.idle":"2024-08-21T04:16:54.236431Z","shell.execute_reply.started":"2024-08-21T04:16:54.230474Z","shell.execute_reply":"2024-08-21T04:16:54.235188Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import Wav2Vec2FeatureExtractor\n\nmodel_name = 'Amulya/wav2vec2-xlsr-english-speech-emotion-recognition'\nconfig_new = AutoConfig.from_pretrained(model_name)\n# processor_new = Wav2Vec2Processor.from_pretrained(model_name)\nprocessor_new = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\nmodel_new = Wav2Vec2ForSpeechClassification.from_pretrained(model_name).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-21T04:16:54.821474Z","iopub.execute_input":"2024-08-21T04:16:54.822201Z","iopub.status.idle":"2024-08-21T04:16:56.195633Z","shell.execute_reply.started":"2024-08-21T04:16:54.822160Z","shell.execute_reply":"2024-08-21T04:16:56.194396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sampling_rate = processor_new.sampling_rate","metadata":{"execution":{"iopub.status.busy":"2024-08-21T04:16:57.397019Z","iopub.execute_input":"2024-08-21T04:16:57.397847Z","iopub.status.idle":"2024-08-21T04:16:57.402593Z","shell.execute_reply.started":"2024-08-21T04:16:57.397805Z","shell.execute_reply":"2024-08-21T04:16:57.401236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction(test.iloc[0])","metadata":{"execution":{"iopub.status.busy":"2024-08-21T04:16:58.465235Z","iopub.execute_input":"2024-08-21T04:16:58.465633Z","iopub.status.idle":"2024-08-21T04:17:00.092360Z","shell.execute_reply.started":"2024-08-21T04:16:58.465604Z","shell.execute_reply":"2024-08-21T04:17:00.091063Z"},"trusted":true},"execution_count":null,"outputs":[]}]}